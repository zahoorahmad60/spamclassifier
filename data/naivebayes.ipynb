{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6NakSXO2Ar9"
      },
      "source": [
        "## Naïve Bayes – Additional Advice\n",
        "This notebook guides you through multiple steps you can follow to create a naïve Bayes classifier. After following these steps you will still need to collate and move your code into the main assignment notebook file so that it meets the required format.\n",
        "\n",
        "Read each step (including the maths!) carefully.\n",
        "\n",
        "You can implement a naïve Bayes classifier without following this advice.\n",
        "\n",
        "This notebook will not be graded and does not need to be submitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PghwSu432AsC",
        "outputId": "3d640b21-d1da-403c-e797-440d5325a26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the spam training data set: (1000, 55)\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]\n",
            " [1. 0. 0. ... 1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "training_spam = np.loadtxt(open(\"/content/training_spam.csv\"), delimiter=\",\")\n",
        "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
        "print(training_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF83o04h2AsD"
      },
      "source": [
        "## The model:  naïve Bayes\n",
        "Your [naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier will distinguish between two classes:\n",
        "\n",
        "* $C = 1$ for spam messages\n",
        "* $C = 0$ for ham messages\n",
        "\n",
        "\n",
        "The classifier builds a model for the probability $p(C=c\\ |\\ \\text{message})$ that a given message belongs to a certain class. A new message is then classified based on the Bayesian *maximum a posteriori* estimate\n",
        "$\\require{color}$\n",
        "\\begin{equation}\n",
        "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\  \\textcolor{blue}{p(C=c\\ |\\ \\text{message})}.\n",
        "\\end{equation}\n",
        "Using Bayes' rule we can write\n",
        "\n",
        "\\begin{equation}\n",
        "p(C=c\\ |\\ \\text{message}) = \\frac{p(\\text{message}\\ |\\ C=c)p(C=c)}{p(\\text{message}\\ |\\ C=1)p(C=1) + p(\\text{message}\\ |\\ C=0)p(C=0)}.  \\quad \\quad\n",
        "\\end{equation}\n",
        "\n",
        "The denominator is the same for both classes and we can thus drop it to get\n",
        "\n",
        "\\begin{equation}\n",
        "\\textcolor{blue}{p(C=c\\ |\\ \\text{message})} \\propto \\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}\\textcolor{green}{p(C=c)},\n",
        "\\end{equation}\n",
        "\n",
        "where $\\propto$ means \"proportional to\". The class priors $\\textcolor{green}{p(C=c)}$ can be computed directly (you will do so in exercise A) but we need to further simplify $\\textcolor{orange}{p(\\text{message} \\ |\\ C=c)}$.\n",
        "\n",
        "\n",
        "### Choice of the event model: *Multinomial* naïve Bayes\n",
        "\n",
        "Different naïve Bayes models differ in their distributional assumptions about $\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}$. We represent a message using a **binary** [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Specifically, a message is represented as a set of $k$ keywords, that is, $message = (w_1, ..., w_k)$, where $w_i = 1$ if the  keyword $w_i$ appears in the message and $w_i = 0$ otherwise.\n",
        "\n",
        "We assume that the $p(w_1, ..., w_k |\\ C=c)$ follows a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) for each class. Don't let the name scare you, this model simply assigns probabilities to different counts of events with multiple outcomes. So for example: \"I roll a biased six-sided die six times, what is the probability that I get each side occurring exactly once\" is a question that can be answered with a multinomial distribution. You don't need to understand all of the equations on the Wikipedia page.\n",
        "\n",
        "Intuitively, the multinomial distribution assumes that the words of a message were \"drawn\" independently from a bag of $k$ different words. Depending on the class membership $c$, each keyword $w$ has a probability $\\theta_{c, w}$ of being drawn. For example,\n",
        "\n",
        "* $\\theta_{spam, w}$ will have high value for $w \\in \\{$bank, transfer, buy,... $\\}$.\n",
        "* $\\theta_{ham, w}$ will have high value for $w \\in \\{$paper, conference, proposal, experiment,... $\\}$, if the training data was mostly gathered from emails of researchers.\n",
        "\n",
        "Under these assumptions, the likelihood of a message, given that it belongs to class $c$, is then proportional to\n",
        "\\begin{equation}\n",
        "\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)} \\propto \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The parameters $\\textcolor{brown}{\\theta_{c, w}}$ are estimated by counting the relative frequencies in the training data. Use **Laplace-smoothing** with $\\alpha = 1$ (add-one smoothing), that is,\n",
        "\\begin{equation}\n",
        "\\textcolor{brown}{\\theta_{c, w}} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
        "\\end{equation}\n",
        "where $n_{c, w}$ is the number of times the keyword $w$ appears in messages of class $c$ in the training set and $n_{c}$ is the total count of keywords for all messages of class $c$, that is, $n_{c} = \\sum_w n_{c, w}$.\n",
        "\n",
        "\n",
        "\n",
        "We are now finally able to rewrite the *maximum a posteriori* estimate in a form that is easy to compute:\n",
        "\\begin{equation}\n",
        "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}\\right].\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "#### Increasing numerical stability\n",
        "We can increase the numerical stability of the algorithm by taking logarithms of the posterior distributions, that is,\n",
        "\\begin{equation}\n",
        "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\log \\left( \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}}\\right)^{w_i} \\right) \\\\\n",
        " = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\log( \\textcolor{green}{p(C=c)}) + \\sum_{i = 1}^k w_i \\ \\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right) \\right].\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4JZ2z4z2AsE"
      },
      "source": [
        "## Part A: Estimate class priors\n",
        "\n",
        "Define a function called `estimate_log_class_priors()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns a numpy array containing the **the logarithm** of the empirical class priors $\\textcolor{green}{p(C=c)}$ for $c \\in \\{0, 1\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vQWw35Uq2AsG"
      },
      "outputs": [],
      "source": [
        "def estimate_log_class_priors(data):\n",
        "    # Extract the binary response variable from the first column\n",
        "    response_variable = data[:, 0]\n",
        "\n",
        "    # Count the occurrences of each class (0s and 1s)\n",
        "    count_class_0 = np.sum(response_variable == 0)\n",
        "    count_class_1 = np.sum(response_variable == 1)\n",
        "\n",
        "    # Calculate the total number of samples\n",
        "    total_samples = len(response_variable)\n",
        "\n",
        "    # Calculate the proportions of each class\n",
        "    proportion_class_0 = count_class_0 / total_samples\n",
        "    proportion_class_1 = count_class_1 / total_samples\n",
        "\n",
        "    # Take the logarithm of the proportions\n",
        "    log_class_prior_0 = np.log(proportion_class_0)\n",
        "    log_class_prior_1 = np.log(proportion_class_1)\n",
        "\n",
        "    # Return the logarithm of class priors as a numpy array\n",
        "    return np.array([log_class_prior_0, log_class_prior_1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "43a82f9670ff38fcd62a636699e1880a",
          "grade": false,
          "grade_id": "cell-e56af38a496339fb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mq-vXYd2AsJ",
        "outputId": "7d977723-146e-4e35-eb32-3a6f42bbe615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result [-0.48939034 -0.94933059]\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
        "log_class_priors = estimate_log_class_priors(training_spam)\n",
        "print(\"result\", log_class_priors)\n",
        "\n",
        "# Check length\n",
        "assert(len(log_class_priors) == 2)\n",
        "\n",
        "# Check whether the returned object is a numpy.ndarray\n",
        "assert(isinstance(log_class_priors, np.ndarray))\n",
        "\n",
        "# Check wehther the values of this numpy.array are floats.\n",
        "assert(log_class_priors.dtype == float)\n",
        "\n",
        "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
        "assert(np.all(log_class_priors < 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhKY338P2AsK"
      },
      "source": [
        "## Part B: Estimate class-conditional likelihoods\n",
        "Define a function called `estimate_log_class_conditional_likelihoods()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns **the logarithm** of the empirical class-conditional likelihoods $\\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)$ for all words $w_i$ and both classes ($c \\in {0, 1}$). These parameters should be returned in a two-dimensional numpy-array with shape = `[num_classes, num_features]`.\n",
        "\n",
        "Assume a multinomial event model and use Laplace smoothing with $\\alpha = 1$.\n",
        "\n",
        "Hint: many `numpy`-functions contain an `axis` argument. If you specify `axis=0`, you can perform column-wise (that is, feature-wise!) computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W9E0662h2AsM"
      },
      "outputs": [],
      "source": [
        "def estimate_log_class_conditional_likelihoods(data, alpha=1.0):\n",
        "    # Extract the binary response variable from the first column\n",
        "    response_variable = data[:, 0].astype(int)  # Convert to integers\n",
        "\n",
        "    # Extract the feature matrix from the remaining columns\n",
        "    features = data[:, 1:]\n",
        "\n",
        "    # Get the number of features\n",
        "    n_features = features.shape[1]\n",
        "\n",
        "    # Initialize arrays to store counts\n",
        "    class_counts = np.zeros((2, n_features))  # Counts of each feature for each class\n",
        "    class_totals = np.zeros(2)  # Total counts for each class\n",
        "\n",
        "    # Iterate over each sample\n",
        "    for i, c in enumerate(response_variable):\n",
        "        # Increment class total count\n",
        "        class_totals[int(c)] += 1  # Convert c to integer\n",
        "        # Increment counts of each feature for the current class\n",
        "        class_counts[int(c)] += features[i]\n",
        "\n",
        "    # Apply Laplace smoothing if alpha > 0\n",
        "    if alpha > 0:\n",
        "        class_counts += alpha\n",
        "        class_totals += alpha * 2  # 2 as there are two classes\n",
        "\n",
        "    # Calculate class-conditional likelihoods\n",
        "    theta = np.log(class_counts / class_totals[:, np.newaxis])\n",
        "\n",
        "    return theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dbfe057bc1b6889411b95a4acf0456ff",
          "grade": false,
          "grade_id": "cell-851fa744923a9bba",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZfcRYV_2AsO",
        "outputId": "a58e9ad2-035f-4671-ddf6-97243b6f07fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.74879343 -2.3107484  -1.28582383 -5.72847509 -1.47997985 -2.26273918\n",
            "  -3.85667291 -2.7080502  -2.43263822 -1.79664945 -2.78403611 -0.84567316\n",
            "  -2.02717311 -3.08941776 -3.93671562 -2.3107484  -2.247235   -2.02717311\n",
            "  -0.58098061 -3.85667291 -1.07451474 -4.6298628  -3.58840892 -3.85667291\n",
            "  -0.9748849  -1.27412779 -1.32175584 -1.85727408 -2.02717311 -1.91076276\n",
            "  -2.29448788 -2.55042126 -2.20211456 -2.52980197 -1.86774538 -1.78689328\n",
            "  -1.25113827 -3.93671562 -2.17312703 -2.3107484  -3.08941776 -2.09088893\n",
            "  -2.247235   -2.29448788 -1.22312524 -1.76766192 -3.93671562 -2.81070436\n",
            "  -1.69423445 -0.59267665 -1.9218126  -1.23983872 -2.26273918 -2.45133035]\n",
            " [-0.97314676 -1.1677888  -0.51684197 -4.01766919 -0.42624508 -0.88217498\n",
            "  -0.90098431 -1.02193692 -1.20998915 -0.74864359 -1.20998915 -0.4705179\n",
            "  -1.13526561 -2.07175905 -1.95624616 -0.56541664 -0.82778091 -1.05092446\n",
            "  -0.11137686 -1.48624253 -0.2110067  -3.01914036 -1.11939226 -0.91372334\n",
            "  -3.47867269 -3.76635477 -5.27043216 -4.17181987 -4.35414143 -4.57728498\n",
            "  -5.27043216 -5.96357934 -3.39862999 -5.96357934 -3.19099062 -2.63137483\n",
            "  -2.78552551 -4.01766919 -3.130366   -1.99328743 -5.96357934 -4.86496705\n",
            "  -2.74470352 -4.35414143 -1.29075051 -3.39862999 -4.17181987 -4.57728498\n",
            "  -1.80469626 -0.45419101 -2.38006041 -0.16146097 -0.46232113 -1.29075051]]\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
        "log_class_conditional_likelihoods = estimate_log_class_conditional_likelihoods(training_spam, alpha=1.0)\n",
        "print(log_class_conditional_likelihoods)\n",
        "\n",
        "# Check data type(s)\n",
        "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
        "\n",
        "# Check shape of numpy array\n",
        "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
        "\n",
        "# Check data type of array elements\n",
        "assert(log_class_conditional_likelihoods.dtype == float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbryZiol2AsR"
      },
      "source": [
        "## Part  C: Classify e-mails\n",
        "\n",
        "Having calculated the log class priors and the log class-conditional likelihoods for a given training set, define a function called `predict()`that takes a data set of new messages as input and predicts for each message whether it is spam or not. Note that the input should **not** contain a response variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w01LIZor2AsT"
      },
      "outputs": [],
      "source": [
        "def predict(new_data, log_class_priors, log_class_conditional_likelihoods):\n",
        "    # Calculate the logarithm of the posterior probability for each class\n",
        "    posterior_log_probs = np.dot(new_data, log_class_conditional_likelihoods.T) + log_class_priors\n",
        "\n",
        "    # Predict the class with the highest posterior probability for each instance\n",
        "    class_predictions = np.argmax(posterior_log_probs, axis=1)\n",
        "\n",
        "    return class_predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "77f96654132ddd809aff99ccad684992",
          "grade": false,
          "grade_id": "cell-4c8adaa150209180",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LLbghXaY2AsV"
      },
      "outputs": [],
      "source": [
        "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
        "class_predictions = predict(training_spam[:, 1:], log_class_priors, log_class_conditional_likelihoods)\n",
        "\n",
        "# Check data type(s)\n",
        "assert(isinstance(class_predictions, np.ndarray))\n",
        "\n",
        "# Check shape of numpy array\n",
        "assert(class_predictions.shape == (1000,))\n",
        "\n",
        "# Check data type of array elements\n",
        "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYObXJs2AsW"
      },
      "source": [
        "Now test your `predict` function by classifying messages. You can do this to the *training* data, but you should also try it on the *testing* data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLQWnBQt2AsX",
        "outputId": "82ac9ffb-e833-4a4c-d26c-561fec85601f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set: 0.801\n"
          ]
        }
      ],
      "source": [
        "# Check accuracy\n",
        "true_classes = training_spam[:, 0]\n",
        "training_set_accuracy = np.mean(np.equal(class_predictions, true_classes))\n",
        "print(f\"Accuracy on the training set: {training_set_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85FKPsqG2AsY"
      },
      "source": [
        "Once you are done, you can move the code into the main assignment notebook.\n",
        "\n",
        "One way to do this is to follow the rough structure of the class that already exists in that notebook. You can use the `train` method to pass in the data and perform all of the steps before the prediction. You should store data in instance variables, e.g. `self.log_class_priors` and `self.log_class_conditional_likelihoods`. This means that then you can set up the `predict` method to match the one above without needing to pass in the additional variables. **Important:** the predict method must only take a single variable as a parameter (the one called `new_data`) in the skeleton code above."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}